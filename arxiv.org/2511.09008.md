# Paper Analysis: A Neurosymbolic Approach to Natural Language Formalization and Verification

**Paper Title:** A Neurosymbolic Approach to Natural Language Formalization and Verification  
**arXiv ID:** 2511.09008v1  
**Paper URL:** https://arxiv.org/pdf/2511.09008

---

## Reading Time Analysis

**Estimated time to read original paper thoroughly:** 60-75 minutes
- 20 pages of dense technical content
- Advanced complexity with formal logic, SMT solvers, and neural-symbolic systems
- Multiple detailed tables, architectural diagrams, and algorithmic descriptions
- Specialized terminology from both AI/ML and formal verification domains
- Mathematical notation and logical formulas throughout

**Estimated time to read this analysis:** 8-10 minutes

**Time savings achieved:** This analysis saves you approximately 60 minutes (7x time reduction)

---

## Step 1: Core Concept (Identify)

**The Problem:**
Large Language Models (LLMs) like ChatGPT are great at understanding and generating natural language, but they sometimes make mistakes or "hallucinate" false information. This is a huge problem for industries like healthcare, finance, and law where accuracy isn't just nice to have—it's legally required. Companies have written policies (rule books) in plain English that tell employees what they can and cannot do, but how do you make sure an AI chatbot follows these rules correctly?

**Why It Matters:**
Imagine an airline using a chatbot to help with refund requests. If the chatbot tells a customer "yes, you get a refund" when the policy actually says "no," the company could face lawsuits and lose customer trust. Current AI systems can't guarantee they won't make these mistakes because they're probabilistic—they guess based on patterns, not logic.

**The Solution:**
This paper introduces ARC (Automated Reasoning Checks), a system that combines LLMs with formal logic (mathematical proofs) to verify that AI-generated answers actually follow the rules in policy documents. It achieves over 99% soundness, meaning when it says an answer is correct, it's correct 99%+ of the time—far better than any existing approach.

---

## Step 2: Teach (Main Contribution & Methodology)

**The Key Innovation:**

Think of ARC like a two-step verification system for AI answers:

**Step 1 - Policy Model Creator (PMC):** The Translation Phase
- Takes a company's policy document written in plain English (like "Seniors over 65 get a 40% discount")
- Uses AI to translate these rules into mathematical logic that computers can understand perfectly
- Creates what they call a "policy model"—essentially a rulebook written in math instead of English
- Allows human experts to review and fix any translation mistakes (like when you proofread an important document)

**Step 2 - Answer Verifier (AV):** The Checking Phase
- When an AI chatbot gives an answer to a customer question, this system checks if it's correct
- Translates the question and answer into the same mathematical language as the policy model
- Uses a "solver" (like a super-precise calculator for logic) to prove whether the answer follows the rules
- Does this check THREE TIMES using different AI models to catch translation errors
- Returns one of several verdicts: Valid (definitely correct), Invalid (definitely wrong), Satisfiable (could be right or wrong depending on missing information), or other categories

**Real-World Example from the Paper:**

A park admission policy says:
- Regular admission: $50
- Low season: 75% of regular price
- Seniors (65+): 40% discount
- If you use credits, discount capped at 25%
- Credits cost $0.60 each, bought in groups of 5
- 10% tax applies to everything

Question: "I'm a senior visiting in low season with $35.40. Can I enter?"
Chatbot Answer: "No, $35.40 is not enough."

ARC's verdict: **Satisfiable** (not Valid!)

Why? Because the answer COULD be right or wrong depending on whether the person uses credits:
- WITHOUT credits: Needs $35.75 (answer is CORRECT)
- WITH credits (buying 15 credits for $9): Needs only $35.34 (answer is WRONG—they CAN enter!)

Even advanced AIs like Claude Opus 4.1 got this wrong, but ARC caught it because it actually did the math.

**How They Tested It:**

They created a dataset by taking existing question-answer pairs and systematically breaking them:
- Removed necessary conditions (Valid → Satisfiable)
- Changed correct conditions to wrong ones (Valid → Invalid)
- Combined contradictory information (Valid → Impossible)

Then they compared ARC to 10 other methods including state-of-the-art AI fact-checkers.

---

## Step 3: Identify Gaps

**Assumptions Not Fully Explained:**

1. **SMT Solver Reliability:** The paper assumes the Z3 SMT solver is always correct, but doesn't discuss what happens if there are bugs in the solver itself or if the logic is at the edge of what's decidable.

2. **Translation Quality Threshold:** While they use 3 LLM calls and majority voting, there's limited justification for why 3 is the right number (not 2 or 5).

3. **Human Vetting Effort:** The paper mentions "several person-hours" for vetting one policy but doesn't provide detailed metrics on how this scales or what expertise is required.

4. **Policy Update Dynamics:** No discussion of how often policies change in real-world settings and the maintenance burden this creates.

**Technical Details Glossed Over:**

1. **Embedding Clustering Algorithm:** They mention using "cosine similarity" to cluster variables but don't specify the clustering algorithm or threshold parameters.

2. **Prompt Engineering Details:** While they provide some prompts in the appendix, the full prompt engineering process and iteration that led to the final prompts isn't documented.

3. **Computational Costs:** Limited discussion of actual API costs, memory requirements, or infrastructure needs for deployment.

4. **Error Propagation:** How do small errors in the policy model formalization compound during answer verification?

**Background Knowledge Assumed:**

1. Basic understanding of formal logic (first-order logic, SMT-LIB syntax)
2. Familiarity with LLM architectures and how they work
3. Understanding of satisfiability checking and theorem proving
4. Knowledge of metrics like precision, recall, F1 (though they do explain soundness)

**Logical Jumps:**

1. The paper jumps from "we use redundant translation" to specific confidence scores without fully explaining the mathematical framework for how three different translations map to a single confidence score.

2. The connection between policy model quality (after human vetting) and final soundness metrics involves multiple confounding factors that aren't fully isolated.

**Unanswered Questions:**

1. How does ARC perform on policies with temporal reasoning (e.g., "within 30 days") or probabilistic statements (e.g., "typically," "usually")?
2. What happens with policies that reference external documents or implicitly assume common knowledge?
3. How well does this approach work with multi-lingual policies or policies that mix languages?
4. What's the failure mode analysis—when ARC is wrong, what patterns of errors occur?
5. Can the system detect when a policy itself is contradictory or incomplete?

---

## Step 4: Simplify

### One-Paragraph Executive Summary (100 words)

ARC is a neurosymbolic guardrail system that achieves 99%+ soundness in verifying whether LLM-generated answers comply with organizational policies—an assurance level unmatched by existing approaches. It works in two stages: first, translating natural language policy documents into formal logic (with optional human review), then verifying answers by translating them into the same logical language and using mathematical proofs to check correctness. By performing redundant translation with multiple LLMs and cross-checking results symbolically, ARC minimizes false approvals while providing actionable feedback to improve answers. This enables reliable AI deployment in regulated industries where accuracy is legally mandated.

### Three Key Takeaways

1. **Two-Stage Architecture:** ARC separates policy formalization (offline, one-time effort with human-in-the-loop) from answer verification (real-time, automated), enabling amortization of upfront costs across many verification tasks while maintaining formal guarantees.

2. **Redundant Translation Strategy:** By translating natural language to logic using three different LLMs and cross-checking semantic equivalence with an SMT solver, ARC achieves 99.2% soundness—detecting ambiguities and increasing confidence beyond what any single LLM can provide.

3. **Conservative by Design:** ARC intentionally trades recall (only 15.6% at highest confidence) for near-zero false positives, reflecting the reality that in regulated domains, falsely approving incorrect content is far more costly than rejecting borderline cases that can be refined or escalated.

### Simple Diagram Description

**Diagram: "The ARC Two-Stage Pipeline"**

*Left Side (Offline - Policy Model Creator):*
- Input: A policy document (like a rulebook)
- Step 1: AI splits document into manageable chunks
- Step 2: Each chunk is translated to mathematical logic rules
- Step 3: All chunks are combined into one unified policy model
- Step 4: Human expert reviews, tests, and fixes any errors
- Output: A verified policy model (rules in mathematical language)

*Right Side (Real-time - Answer Verifier):*
- Input: A question and AI-generated answer
- Step 1: Three different AIs independently translate the Q&A into logic
- Step 2: A "logic calculator" checks if all three translations agree
- Step 3: The same calculator proves whether the answer follows the policy rules
- Output: A verdict (Valid/Invalid/Satisfiable/etc.) with explanation

*Arrow connecting both sides:* The policy model feeds into the Answer Verifier

*Color coding:* 
- Blue boxes = AI/LLM steps (probabilistic)
- Green boxes = Mathematical proof steps (deterministic)
- Orange boxes = Human involvement (optional but improves quality)

### Core Analogy

**ARC is like a legal system with two phases:**

**Phase 1 - Creating the Law (PMC):**
Think of lawmakers writing laws. They start with ideas in everyday language ("people should drive safely"), but eventually these must be codified into precise legal text with specific definitions and rules. Sometimes the original language is ambiguous, so legal experts must clarify what was really meant. This happens once, and the resulting law book becomes the official reference.

**Phase 2 - Enforcing the Law (AV):**
When someone commits an action, judges must determine if it violated the law. To be absolutely certain, you might have three independent judges review the case (redundant translation) and only convict if all three agree on what happened. Then, you use formal legal reasoning to prove whether the action violated the specific statutes. If there's any ambiguity in interpreting what happened, the case might be "satisfiable" (could go either way) rather than definitively "valid" or "invalid."

The key insight: You want **near-zero false convictions** (soundness), even if that means letting some borderline cases go free (lower recall). In regulated industries, a false approval is like convicting an innocent person—the consequences are severe.

### The "So What?" - Real-World Impact

**Why This Matters:**

1. **Regulatory Compliance:** Healthcare companies processing insurance claims, banks handling loan applications, airlines managing refunds—all have strict legal obligations. ARC enables them to deploy AI assistants while maintaining legal accountability because they can prove the AI follows company policies.

2. **Trust and Liability:** When an AI chatbot gives advice, who's liable if it's wrong? ARC provides an auditable paper trail showing exactly which policy rules were checked and whether the answer was mathematically proven correct. This is crucial for both internal governance and external audits.

3. **Cost Savings with Guarantees:** Companies want to automate customer service but can't afford the lawsuits from AI mistakes. ARC's 99%+ soundness means they can deploy automation safely—the remaining 1% can be handled by human escalation.

4. **Better Than Human Consistency:** Humans make mistakes, especially with complex policies involving calculations (like the park admission example). ARC doesn't get tired, doesn't misread numbers, and applies rules consistently every single time.

5. **Bridge to Smarter AI:** As LLMs improve, ARC will automatically benefit (better translations), while maintaining its mathematical guarantees. This creates a path toward increasingly autonomous systems in regulated industries.

**Concrete Example:**
A health insurance company could use ARC to verify that their AI assistant correctly applies coverage policies. If the AI says "your surgery is covered," ARC proves this mathematically against the policy. If verification fails, the case is flagged for human review. This prevents both false denials (bad for patients) and false approvals (bad for the company's finances).

---

## Critical Analysis

### Strengths (3 points)

1. **Unprecedented Soundness Guarantee:** Achieving 99%+ soundness represents a genuine breakthrough. The paper clearly demonstrates that existing approaches (LLM-as-judge, fact-checkers) max out around 94-98% soundness, making ARC the first to cross the critical "two nines" threshold. This isn't incremental—it's the difference between "pretty good" and "deployable in regulated industries."

2. **Rigorous Neurosymbolic Design:** The separation between policy formalization (PMC) and answer verification (AV) is architecturally elegant. By investing human effort once during policy creation and amortizing it across many verifications, they solve the scalability problem that plagues pure symbolic approaches. The redundant translation mechanism with semantic equivalence checking is a clever solution to the "translation uncertainty" problem.

3. **Actionable Feedback Loop:** Unlike binary fact-checkers that just say "true/false," ARC provides rich feedback (counter-examples, satisfying assignments, relevant rules) that helps both humans understand why something failed and LLMs iteratively improve their outputs. The paper demonstrates this with Figure 4, showing answers improving from 10.8% to 43.9% valid through iterative refinement.

### Weaknesses/Limitations (3 points)

1. **Scalability Concerns Underexplored:** While the paper shows linear scaling of policy model size with document length (Appendix A.1.3), it glosses over the human vetting burden. The case study mentions "several person-hours" for one airline policy, but real enterprise policies can be hundreds of pages with thousands of rules. The paper doesn't adequately address how organizations should approach vetting at this scale or provide tooling to make it more efficient.

2. **Severe Recall-Soundness Tradeoff:** At the strictest threshold (3/3 ensemble), ARC only accepts 15.6% of valid content as valid. This means 84.4% of perfectly correct answers get rejected or flagged as uncertain. While the paper justifies this as "conservative by design," it raises questions about practical deployment: if only 1 in 6 correct answers is auto-approved, is this really automating the task, or just creating a different kind of human review workload?

3. **Limited Formalism Expressiveness:** The paper restricts to quantifier-free SMT-LIB with non-linear arithmetic (QF_NRIA), which can't naturally express temporal logic ("within 30 days of purchase"), probabilistic reasoning ("typically," "usually"), or modal concepts ("must," "may," "should" with different legal weights). Many real-world policies use this language, but the paper doesn't discuss how to handle these cases or what percentage of real policies fall outside the expressible fragment.

### Relationship to Broader Field

**Positioning in Neurosymbolic AI:**
ARC represents a pragmatic middle ground in the neurosymbolic research spectrum. Unlike pure symbolic approaches (theorem provers, rule engines) that demand perfect formal specifications upfront, ARC uses LLMs to bootstrap formalization. Unlike pure neural approaches (LLM-as-judge, fact-checkers) that remain probabilistic, ARC grounds verification in mathematical proof. This positions it as a "best of both worlds" solution for a specific use case (policy compliance) rather than a general-purpose reasoning system.

**Connection to Formal Verification:**
The paper brings ideas from software verification (model checking, SMT solving) into the LLM era. The concept of "soundness" as a primary metric, the focus on false positive minimization, and the use of SMT solvers for proof generation are all borrowed from formal methods. However, unlike traditional formal verification which assumes perfect specifications, ARC must handle the messy reality of natural language policies with human-in-the-loop refinement.

**Relationship to AI Safety and Alignment:**
While not explicitly framed as an AI safety paper, ARC addresses a core alignment problem: how do we ensure AI systems follow specified rules? The paper's focus on "guardrails" and "verifiable guarantees" aligns with the broader goal of building trustworthy AI. However, it's solving a narrower problem (following explicit policies) rather than the full alignment challenge (understanding and following human values generally).

**Impact on Enterprise AI Adoption:**
This work directly addresses a major barrier to enterprise AI adoption in regulated industries. By providing mathematical proofs of compliance, ARC could accelerate deployment in healthcare, finance, legal services, and government—sectors that have been slower to adopt generative AI due to liability concerns. The paper's real-world policy experiments (airline refunds, insurance policies) demonstrate awareness of practical deployment requirements.

### Potential Follow-up Questions and Research Directions

1. **Automated Policy Vetting:** Can we develop AI-assisted tools that automatically detect common policy model errors (missing variables, incomplete rules, ambiguities) and suggest fixes, reducing the "several person-hours" vetting burden to minutes?

2. **Adaptive Confidence Thresholds:** Instead of a fixed 2/3 or 3/3 threshold, can the system learn appropriate confidence levels for different types of questions or policy domains? High-stakes medical decisions might require 3/3, while simple FAQ answers could use 1/3.

3. **Richer Logical Formalisms:** Extend beyond QF_NRIA to support temporal logic (LTL/CTL), probabilistic reasoning (Bayesian networks), and deontic logic (obligations, permissions, prohibitions). What percentage of real policies require these extensions?

4. **Cross-Policy Reasoning:** How do you handle situations where multiple policies interact (e.g., company policy + regulatory requirements + contractual obligations)? Can ARC detect conflicts between policies?

5. **Fine-tuned Translation Models:** The paper uses off-the-shelf LLMs for formalization. Could specialized models trained on policy-to-logic translation pairs achieve higher accuracy and lower latency? What's the learning curve for such a model?

6. **Continuous Learning from Feedback:** When ARC flags an issue and a human corrects it, can this feedback be used to automatically improve the policy model or the translation process? Create a virtuous cycle of improvement?

7. **Adversarial Robustness:** How robust is ARC to adversarial examples where users deliberately try to trick the system with carefully crafted questions? Can prompt injection attacks bypass the formal verification?

8. **Explainability for Non-Experts:** The current explanations involve SMT-LIB formulas and logic assignments. Can we generate natural language explanations that non-technical stakeholders (customers, regulators) can understand?

9. **Cost-Benefit Analysis:** What's the total cost of ownership (TCO) for deploying ARC vs. alternatives? Factor in API costs (3 LLM calls per verification), human vetting time, maintenance, and false rejection handling.

10. **Three Nines and Beyond:** The paper targets "two nines" (99%) soundness as a milestone. What architectural changes would be needed to reach "three nines" (99.9%) or higher? Is there a theoretical limit?

---

## Technical Deep Dive

### Key Algorithms Simplified

**Algorithm 1: Redundant Translation (Core of Answer Verifier)**

```
Input:
  - msg: Natural language question and answer
  - policy: The formal policy model (rules in logic)
  - LLMs: List of k language models (typically k=3)

Process:
1. For each LLM, independently translate msg into logic:
   - Identify premises (P): contextual facts from the question
   - Identify conclusions (C): claims made in the answer
   - Express both as formulas using policy variables

2. For each premise-conclusion pair <P,C> from any translation:
   - Count how many of the k translations support P ⇒ C
   - Support means: the translation logically implies P→C
                    AND the premises aren't self-contradictory
   - Confidence = (count of supporting translations) / k

3. Return all unique <P, C, confidence> tuples

Example:
- If all 3 LLMs produce identical translations: confidence = 3/3 = 1.0
- If 2 LLMs agree, 1 differs: two findings, confidences 2/3 and 1/3
- If all 3 differ: three findings, each with confidence 1/3
```

**Why This Works:**
- Different LLMs have different "views" of the natural language
- If they all agree on the logic, we're more confident the translation is correct
- If they disagree, we've detected ambiguity—flag for human review
- The SMT solver checks semantic equivalence (not just string matching)

**Policy Model Creator (High-Level)**

```
Input: Natural language policy document

Phase 1: Divide and Conquer
1. Split document into text spans (manageable chunks)
2. For each span:
   a. LLM identifies formalizable statements
   b. LLM translates to datatypes, variables, and rules (SMT-LIB)
   c. If syntax errors, give feedback to LLM for repair
   d. Result: one "policy unit" per span

Phase 2: Composition
3. Generate embeddings for all variables across units
4. Cluster similar variables using cosine similarity
5. Merge variables within clusters (unify semantics)
6. Rename variables that have same name but different meanings
7. Aggregate all rules, removing duplicates
8. Output: unified policy model

Phase 3: Vetting (optional human-in-the-loop)
9. Lint: check for unused variables, contradictory rules
10. Inspect: human reviews rules and descriptions
11. Test: generate or run test cases
12. Repair: fix issues found during vetting
```

**SMT-LIB Fragment (Logical Language)**

The paper uses a restricted fragment of SMT-LIB:
- Data types: Int (integers), Real (decimals), Bool (true/false), Enums (named values)
- Operations: +, -, *, /, =, <, >, ≤, ≥, and, or, not, implies (=>)
- NO quantifiers (∀, ∃) - all variables are concrete
- Allows non-linear arithmetic (like x*y + z)

Example translation:
```
English: "Seniors over 65 get a 40% discount"

SMT-LIB:
(declare-const age Int)
(declare-const isSenior Bool)
(declare-const discountRate Real)

(assert (=> (> age 65) (= isSenior true)))
(assert (=> (= isSenior true) (= discountRate 0.40)))
```

### Critical Experimental Results

**Table 1: CONDITIONALQA-LOGIC Benchmark**

Key Finding: ARC achieves 99.2% soundness vs. 98.3% for next-best (LLMaJ ensemble)

Translation:
- Out of 1,566 total test cases, ARC incorrectly labeled only 13 as valid when they weren't
- Next best method (LLMaJ) had 26 false positives—twice as many
- Traditional fact-checkers like RefChecker had 245 false positives—19x worse

But: ARC's recall is only 15.6% (detected 163 out of 1,047 truly valid cases)

Trade-off Analysis:
- At 3/3 confidence: soundness=99.2%, recall=15.6% (ultra-conservative)
- At 2/3 confidence: soundness=98.7%, recall=20.3% (slightly more permissive)
- Without redundancy: soundness=98.6%, recall=31.7% (single LLM translation)

**Interpretation:**
The redundant translation mechanism adds only ~0.6% soundness improvement but cuts recall in half. This suggests the main value isn't in catching errors, but in detecting ambiguity (translation disagreement) and refusing to make a judgment when uncertain.

**Table 2: Human Vetting Impact (RyanAir Case Study)**

Key Finding: Human vetting increased soundness from 96.8% to 100%, recall from 25% to 45.5%

Numbers:
- Before vetting: 2 false positives out of 62 tests
- After vetting: 0 false positives, and 9 more true positives

**Interpretation:**
The autoformalization process is good but not perfect. Human vetting both reduces errors (soundness) and captures nuances that improve coverage (recall). The case study revealed specific issues:
- Ambiguity about "entitled to refund if delayed 5+ hours" (is not traveling a prerequisite?)
- Exception handling (broad rule with exceptions stated elsewhere)
- Edge cases (death *on* travel day vs. *before*)

**Figure 4: Iterative Answer Refinement**

Key Finding: With ARC feedback, LLM-revised answers improved from 10.8% valid to 43.9% valid after 3 iterations

Breakdown:
- Most improvement came from fixing "Satisfiable" answers (47% → 9% after 3 iterations)
- These are answers missing necessary conditions
- ARC provides a counter-example showing what's missing
- LLM uses this to add the missing context

But: "TranslationAmbiguous" and "NoTranslations" didn't improve much

**Interpretation:**
ARC's feedback is most effective when the issue is incompleteness (missing conditions) rather than policy model inadequacy (missing variables, ambiguous definitions). This suggests a future direction: use verification feedback to improve the policy model itself, not just the answers.

### Statistical Significance and Validation Methods

**Dataset Construction:**

CONDITIONALQA-LOGIC: 522 test cases
- 349 Valid (ground truth: correct answers)
- 173 Not Valid (various error types)
  - 103 Invalid (wrong conditions)
  - 52 Satisfiable (missing conditions)
  - 4 Impossible (contradictory premises)
  - 14 NoTranslations (not answerable)

The dataset was created systematically by manipulating original ConditionalQA examples, ensuring balanced representation of error types. This is more rigorous than random sampling because it targets specific failure modes.

**Baseline Comparisons:**

The paper compares against 10 different methods:
- 3 variants of LLM-as-Judge (ensemble vs. single, standard vs. extended thinking)
- 3 variants of FACTS Grounding (different output formats)
- 4 specialized fact-checkers (MiniCheck, RefChecker, SelfCheckGPT, Logic-LM)

All methods used the same test set and comparable configurations (same base LLM, same prompts where applicable). This is a fair comparison.

**Confidence Intervals Not Provided:**

A weakness: The paper reports point estimates (99.2% soundness) without confidence intervals or statistical significance tests. With 519 negative cases and 13 false positives, the 95% confidence interval for false positive rate would be approximately 1.8%-4.2%, meaning the true soundness is likely between 95.8% and 98.2%—still industry-leading but with some uncertainty.

**Cross-Dataset Generalization:**

The paper tests on two types of data:
1. CONDITIONALQA-LOGIC: synthetic, based on existing Q&A dataset
2. Real-world policies: 6 company policy documents (insurance, airline refunds, etc.)

Results on real-world policies (Table 2) showed 100% soundness after vetting, but on a much smaller test set (62 cases). This limits confidence in generalization.

**Threat to Validity: Overfitting to Test Set?**

The paper states "Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity" and emphasizes the method wasn't trained on the test data. However:
- The prompts and system design were likely iteratively refined based on performance
- The choice of logical formalism (QF_NRIA) was made to fit the kinds of policies tested
- It's unclear if performance would hold on policies from different domains (legal contracts, medical protocols)

### Robustness Analysis

**What Makes Conclusions Robust:**

1. **Multiple Evaluation Metrics:** The paper reports soundness, FPR, precision, recall, F1, and accuracy. ARC dominates on soundness/FPR while acknowledging the recall tradeoff. This honest reporting of strengths and limitations increases credibility.

2. **Ablation Studies:** Table 1 rows 1-3 show performance with/without redundant translation, demonstrating that each component contributes. This rules out the possibility that one component is doing all the work.

3. **Diverse Baselines:** Comparing against both neural (LLM-as-judge) and neurosymbolic (Logic-LM) methods shows ARC's advantage isn't just "we used formal methods" but specifically the two-stage architecture with redundant translation.

4. **Real-World Case Study:** The RyanAir vetting example (Section 4.2) provides qualitative insights into what kinds of errors occur and how human vetting helps. This grounds the quantitative results in practical reality.

**What Limits Conclusions:**

1. **Small-Scale Real-World Testing:** Only 6 policy documents and 62 test cases after human vetting. This is orders of magnitude smaller than what would be needed to confidently claim "ready for production deployment."

2. **Domain-Specific:** All tested policies are in the business/commercial domain (refunds, insurance, admissions). Performance on legal contracts, medical protocols, or government regulations is unknown.

3. **Optimal Attack Surface Not Explored:** The paper doesn't test adversarial examples where someone deliberately tries to fool the system. Real deployment would face users attempting prompt injection or logic exploits.

4. **Human Vetting Not Scaled:** One detailed case study isn't enough to understand the full learning curve, time requirements, or expertise needed for vetting across diverse policy types.

5. **Cost Not Quantified:** Three LLM calls per verification is expensive. At scale (millions of customer interactions), this could cost thousands of dollars per day. No cost-benefit analysis is provided.

**Overall Assessment:**

The core claim—that ARC achieves 99%+ soundness through neurosymbolic reasoning—is well-supported by the evidence. However, the practical deployment claims should be taken as "promising research direction" rather than "ready for production." More extensive real-world testing, adversarial evaluation, and cost analysis would be needed before enterprise adoption.